{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theory\n",
    "## Q1\n",
    "### What is the difference between normal and deformable convolutional networks in grid sampling?\n",
    "The main difference between normal convolutional networks and deformable convolutional networks (DCNs) lies in the way they perform grid sampling during the convolution operation. In traditional convolutional networks, a fixed regular grid is used to sample input features within the receptive field of each convolutional kernel. This fixed grid, however, may not be optimal for capturing deformable and complex patterns in images.\n",
    "\n",
    "On the other hand, deformable convolutional networks introduce the concept of deformable convolutional layers, where the grid sampling is adaptive and dynamic. In DCNs, each sampling point within the convolutional kernel is associated with learnable offsets, which determine how the point should be adjusted or deformed during the convolution operation. This adaptability enables the network to focus on more relevant areas of the input feature map, particularly those corresponding to deformable objects or regions with intricate spatial structures.\n",
    "\n",
    "In summary, the key distinction is that normal convolutional networks use a fixed grid for sampling input features, whereas deformable convolutional networks incorporate learnable offsets to dynamically adjust the sampling points, allowing for increased flexibility and adaptability in capturing complex spatial relationships and deformable structures within images. This adaptability is particularly beneficial for tasks such as object detection and semantic segmentation where objects may vary in shape, scale, and pose.\n",
    "\n",
    "## Q2\n",
    "### How can deformable convolutional networks handle flexibility in images with geometric transformations?\n",
    "Deformable Convolutional Networks (DCNs) are designed to handle flexibility in images with geometric transformations by introducing adaptive and learnable deformations within the convolutional layers. This adaptability allows the network to effectively capture and model complex spatial relationships, particularly in the presence of deformable objects and geometric transformations. Here's how DCNs achieve this:\n",
    "\n",
    "1. **Learnable Offsets:** In a deformable convolutional layer, each sampling point within the convolutional kernel is associated with learnable offsets. These offsets are predicted based on the input features and indicate how the sampling points should be adjusted. By learning these offsets, the network gains the ability to dynamically modify the sampling grid, enabling it to focus on relevant areas of the input feature map.\n",
    "\n",
    "2. **Adaptive Receptive Fields:** The learnable offsets allow the receptive field of each convolutional kernel to be adaptively adjusted. This adaptability is crucial when dealing with objects that undergo geometric transformations, such as changes in scale, rotation, or deformation. The network can effectively adapt its receptive field to capture relevant information in transformed regions, improving its ability to recognize and understand objects in varying configurations.\n",
    "\n",
    "3. **Spatial Deformation:** The deformable convolutional layer includes a deformation module that warps the input features based on the predicted offsets. This spatial deformation introduces geometric transformations to the feature map, aligning the network's sampling points with the underlying structure of the input. This mechanism is particularly beneficial in handling deformable objects and complex spatial variations that may arise due to geometric transformations.\n",
    "\n",
    "4. **Improved Localization Accuracy:** The adaptive nature of DCNs enhances the network's localization accuracy, especially in tasks like object detection and semantic segmentation. The ability to adaptively sample from different locations allows the network to precisely locate object boundaries and handle variations in object shapes and positions caused by geometric transformations.\n",
    "\n",
    "By incorporating these mechanisms, DCNs provide a more flexible and robust approach to handling geometric transformations in images. This adaptability is crucial in real-world scenarios where objects may appear in different poses, scales, or orientations, making deformable convolutional networks well-suited for a variety of computer vision tasks.\n",
    "\n",
    "## Q3\n",
    "### Why normal convolutional networks face errors when dealing with images with spatial rotation or translations?\n",
    "Normal convolutional networks may face challenges when dealing with images containing spatial rotations or translations because of their fixed and rigid grid sampling strategy. In a standard convolutional layer, the receptive field is defined by a fixed grid of sampling points, and each point within the convolutional kernel samples the input feature map at a predetermined location. This fixed grid assumption makes traditional convolutional networks less robust to spatial transformations. Here's why:\n",
    "\n",
    "1. **Rigid Grid Sampling:** In a regular convolutional layer, the grid sampling is rigid and does not adapt to spatial transformations. As a result, when an image undergoes translation or rotation, the fixed grid may not align optimally with the transformed features, leading to misalignments between the learned filters and the transformed object structures. This misalignment can cause a loss of information and hinder the network's ability to recognize objects accurately.\n",
    "\n",
    "2. **Limited Receptive Field:** The fixed grid limits the receptive field's adaptability to changes in the spatial configuration of objects. As objects undergo rotations or translations, important information may fall outside the fixed receptive field, making it difficult for the network to capture and understand the transformed features effectively.\n",
    "\n",
    "3. **Inability to Capture Deformations:** Traditional convolutional networks are less equipped to handle deformable structures or objects with complex spatial variations caused by rotations or translations. The fixed sampling grid does not allow the network to dynamically adjust its sampling points to capture these deformations, leading to reduced performance in tasks requiring accurate localization and recognition of such objects.\n",
    "\n",
    "In contrast, deformable convolutional networks (DCNs) address these limitations by introducing learnable offsets associated with each sampling point. These offsets enable adaptive and deformable sampling, allowing the network to better align with transformed features. DCNs, with their ability to dynamically adjust receptive fields and capture spatial deformations, are more resilient to errors caused by spatial rotations or translations. This makes them well-suited for tasks where objects may undergo geometric transformations, such as object detection and recognition in images with diverse spatial configurations.\n",
    "\n",
    "## Q4\n",
    "### How are offsets in deformable convolutional networks calculated?\n",
    "In Deformable Convolutional Networks (DCNs), the offsets are calculated through a learnable offset prediction module. This module generates a set of learnable parameters that determine how the sampling points within the convolutional kernel should be adjusted or deformed during the convolution operation. The process involves the following steps:\n",
    "\n",
    "1. **Offset Prediction:** The offset prediction module takes the input feature map as its input. It computes the learnable offsets for each sampling point within the convolutional kernel. These offsets are typically represented as 2D vectors (for 2D convolution) and indicate the spatial adjustments for each sampling point.\n",
    "\n",
    "2. **Learnable Parameters:** The learnable parameters associated with the offset prediction module are trained during the network's training phase. These parameters are updated through backpropagation during the optimization process, allowing the network to learn the optimal adjustments for different features and tasks.\n",
    "\n",
    "3. **Adjustment of Sampling Points:** The calculated offsets are then used to adjust the sampling points within the convolutional kernel during the convolution operation. This adjustment introduces a form of spatial deformation to the input feature map, enabling the network to adaptively sample features from different locations based on the learned offsets.\n",
    "\n",
    "4. **Sampling with Deformation:** The deformed sampling points are used to extract features from the input feature map. This dynamic sampling mechanism allows the network to focus on relevant areas of the input, capturing spatial relationships and deformable structures more effectively than traditional convolutional layers with fixed grids.\n",
    "\n",
    "By incorporating learnable offsets, DCNs introduce adaptability to the convolutional layers, allowing the network to better handle spatial variations, deformations, and complex structures in images. The offset prediction module is a crucial component of DCNs, and the learnable nature of the offsets enables the network to generalize well to different tasks and types of images during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification\n",
    "in this part we'll use CIFAR10 dataset to compare the performance of Deformable Convolutional Networks with the original Convolutional Networks. We'll use the ResNet34 as the backbone network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import CIFAR10\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataset = CIFAR10(root='./data', train=True, download=True, transform=torchvision.transforms.ToTensor())\n",
    "test_dataset = CIFAR10(root='./data', train=False, download=True, transform=torchvision.transforms.ToTensor())\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class Conv2D(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0):\n",
    "        super(Conv2D, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "class DeformableConv2D(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_channels,\n",
    "                 out_channels,\n",
    "                 kernel_size=3,\n",
    "                 stride=1,\n",
    "                 padding=0,\n",
    "                 bias=False):\n",
    "        super(DeformableConv2D, self).__init__()\n",
    "        kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.offset_conv = nn.Conv2d(in_channels, 2 * kernel_size * kernel_size, kernel_size=kernel_size, stride=stride, padding=self.padding, bias=True)\n",
    "        self.modulator_conv = nn.Conv2d(in_channels,\n",
    "                                        1 * kernel_size * kernel_size,\n",
    "                                        kernel_size=kernel_size,\n",
    "                                        stride=stride,\n",
    "                                        padding=self.padding,\n",
    "                                        bias=True)\n",
    "        self.regular_conv = nn.Conv2d(in_channels=in_channels,\n",
    "                                      out_channels=out_channels,\n",
    "                                      kernel_size=kernel_size,\n",
    "                                      stride=stride,\n",
    "                                      padding=self.padding,\n",
    "                                      bias=bias)\n",
    "        nn.init.constant_(self.offset_conv.weight, 0.)\n",
    "        nn.init.constant_(self.offset_conv.bias, 0.)\n",
    "        nn.init.constant_(self.modulator_conv.weight, 0.)\n",
    "        nn.init.constant_(self.modulator_conv.bias, 0.)\n",
    "\n",
    "    def forward(self, x):\n",
    "        offset = self.offset_conv(x)\n",
    "        modulator = 2. * torch.sigmoid(self.modulator_conv(x))\n",
    "        x = torchvision.ops.deform_conv2d(input=x,\n",
    "                                          offset=offset,\n",
    "                                          weight=self.regular_conv.weight,\n",
    "                                          bias=self.regular_conv.bias,\n",
    "                                          padding=self.padding,\n",
    "                                          mask=modulator,\n",
    "                                          stride=self.stride)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1, downsample=None, conv=Conv2D):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = conv(in_channels, out_channels, kernel_size=3, stride=stride, padding=1)\n",
    "        self.conv2 = conv(out_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.downsample = downsample\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.conv2(out)\n",
    "        if self.downsample:\n",
    "            residual = self.downsample(x)\n",
    "        out = out + residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, conv, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.conv1 = conv(3, 64, kernel_size=7, stride=2, padding=3)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, conv, 64, 64, 3)\n",
    "        self.layer2 = self._make_layer(block, conv, 64, 128, 4, stride=2)\n",
    "        self.layer3 = self._make_layer(block, conv, 128, 256, 6, stride=2)\n",
    "        self.layer4 = self._make_layer(block, conv, 256, 512, 3, stride=2)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512, num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def _make_layer(self, block, conv, in_channels, out_channels, block_num, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            downsample = conv(in_channels, out_channels, kernel_size=1, stride=stride, padding=0)\n",
    "        layers = []\n",
    "        layers.append(block(in_channels, out_channels, stride, downsample, conv))\n",
    "        for i in range(1, block_num):\n",
    "            layers.append(block(out_channels, out_channels, conv=conv))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_net = ResNet(ResidualBlock, Conv2D)\n",
    "deformable_net = ResNet(ResidualBlock, DeformableConv2D)\n",
    "\n",
    "normal_net = normal_net.to(device)\n",
    "deformable_net = deformable_net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "normal_optimizer = optim.Adam(normal_net.parameters(), lr=0.001)\n",
    "deformable_optimizer = optim.Adam(deformable_net.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def train(model, optimizer, criterion, train_loader):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    pbar = tqdm(train_loader)\n",
    "    for i, data in enumerate(pbar):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "        pbar.set_description(\"loss: {:.4f}\".format(np.mean(losses)))\n",
    "    return losses\n",
    "\n",
    "def test(model, criterion, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            test_loss += criterion(outputs, labels).item() * inputs.size(0)\n",
    "            pred = outputs.max(1, keepdim=True)[1]\n",
    "            correct += pred.eq(labels.view_as(pred)).sum().item()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    return test_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_losses = []\n",
    "deformable_losses = []\n",
    "normal_accuracy = []\n",
    "deformable_accuracy = []\n",
    "\n",
    "for epoch in range(1, 11):\n",
    "    print(\"epoch {}\".format(epoch))\n",
    "    loss = train(normal_net, normal_optimizer, criterion, train_loader)\n",
    "    normal_losses.extend(loss)\n",
    "    loss, accuracy = test(normal_net, criterion, test_loader)\n",
    "    normal_accuracy.append(accuracy)\n",
    "    print('Test set: Average loss: {:.4f}, Accuracy: {:.4f}'.format(loss, accuracy))\n",
    "\n",
    "    loss = train(deformable_net, deformable_optimizer, criterion, train_loader)\n",
    "    deformable_losses.extend(loss)\n",
    "    loss, accuracy = test(deformable_net, criterion, test_loader)\n",
    "    deformable_accuracy.append(accuracy)\n",
    "    print('Test set: Average loss: {:.4f}, Accuracy: {:.4f}'.format(loss, accuracy))\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(np.arange(1, 11), normal_accuracy, label='normal')\n",
    "plt.plot(np.arange(1, 11), deformable_accuracy, label='deformable')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sum(p.numel() for p in normal_net.parameters() if p.requires_grad))\n",
    "print(sum(p.numel() for p in deformable_net.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "- Deformable train accuracy: 0.86\n",
    "- Normal train accuracy: 0.82\n",
    "- Deformable test accuracy: 0.84\n",
    "- Normal test accuracy: 0.76\n",
    "\n",
    "Deformable results in more generalization and better performance with a little parameter overhead. Although each iteration costs more. Each epoch lasted nearly 1:30 while the normal one lasted 0:45 which is nearly half of the deformable one. But the results are worth it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic Segmentation\n",
    "in this part we'll use MS COCO dataset to compare the performance of Deformable Convolutional Networks with the original Convolutional Networks. We'll use the U-Net as the backbone network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, conv=Conv2D):\n",
    "        super(EncoderBlock, self).__init__()\n",
    "        self.conv1 = conv(in_ch, out_ch, 3, stride=1, padding=1)\n",
    "        self.conv2 = conv(out_ch, out_ch, 3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        return self.pool(x), x\n",
    "\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, conv=Conv2D):\n",
    "        super(DecoderBlock, self).__init__()\n",
    "\n",
    "        self.upconv = nn.ConvTranspose2d(in_ch, out_ch, kernel_size=2, stride=2)\n",
    "        self.upbn = nn.BatchNorm2d(out_ch)\n",
    "        self.conv1 = conv(in_ch + out_ch, out_ch, 3, stride=1, padding=1)\n",
    "        self.conv2 = conv(out_ch, out_ch, 3, stride=1, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x, features):\n",
    "        x = self.relu(self.upbn(self.upconv(x)))\n",
    "        x = self.conv1(torch.cat([x, features], dim=1))\n",
    "        x = self.conv2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, classes=90, conv=Conv2D):\n",
    "        super(UNet, self).__init__()\n",
    "\n",
    "        self.encoders = nn.ModuleList([\n",
    "            EncoderBlock(3, 64, conv),\n",
    "            EncoderBlock(64, 128, conv),\n",
    "            EncoderBlock(128, 256, conv),\n",
    "        ])\n",
    "\n",
    "        self.decoders = nn.ModuleList([\n",
    "            DecoderBlock(256, 128, conv),\n",
    "            DecoderBlock(128, 64, conv),\n",
    "            DecoderBlock(64, classes, conv),\n",
    "        ])\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        self.out = nn.Sigmoid(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        enc_features = []\n",
    "        for i in range(len(self.encoders)):\n",
    "            x, features = self.encoders[i](x)\n",
    "            enc_features.append(features)\n",
    "        enc_features = enc_features[::-1]\n",
    "        for i in range(len(self.decoders)):\n",
    "            x = self.decoders[i](x, enc_features[i])\n",
    "            if i < len(self.decoders) - 1:\n",
    "                x = self.relu(x)\n",
    "        return self.out(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unet_normal = UNet(Conv2D)\n",
    "unet_deformable = UNet(DeformableConv2D)\n",
    "\n",
    "unet_normal = unet_normal.to(device)\n",
    "unet_deformable = unet_deformable.to(device)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "unet_normal_optimizer = optim.Adam(unet_normal.parameters(), lr=0.001)\n",
    "unet_deformable_optimizer = optim.Adam(unet_deformable.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coco_train = torchvision.datasets.CocoDetection(root=\"./train2017\", annFile=\"./annotations/instances_train2017.json\", transform=torchvision.transforms.ToTensor())\n",
    "coco_val = torchvision.datasets.CocoDetection(root=\"./val2017\", annFile=\"./annotations/instances_val2017.json\", transform=torchvision.transforms.ToTensor())\n",
    "\n",
    "train_loader = DataLoader(coco_train, batch_size=1, shuffle=True)\n",
    "val_loader = DataLoader(coco_val, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_img(idx):\n",
    "  img, target = coco_train[idx]\n",
    "  img = np.array(img.permute(1, 2, 0))\n",
    "  plt.imshow(img)\n",
    "  plt.show()\n",
    "  plt.imshow(img)\n",
    "  for box in target:\n",
    "      x, y, width, height = box[\"bbox\"]\n",
    "      seg = box[\"segmentation\"][0]\n",
    "      plt.fill(seg[0::2], seg[1::2], linewidth=2, alpha=0.7)\n",
    "      category_id = box[\"category_id\"]\n",
    "      category = coco_train.coco.cats[category_id][\"name\"]\n",
    "      plt.text(x + width / 2, y + height / 2, category, color=\"red\", ha=\"center\", va=\"center\")\n",
    "  plt.show()\n",
    "\n",
    "plot_img(np.random.randint(0, len(coco_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_coco_semantic_segmentation(model, optimizer, criterion, train_loader):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    pbar = tqdm(train_loader)\n",
    "    for i, data in enumerate(pbar):\n",
    "        img, target = data\n",
    "        img, target = img.to(device), target.to(device)\n",
    "        label = torch.stack([torch.zeros_like(img)] * 90, dim=1)\n",
    "        for box in target:\n",
    "            x, y, width, height = box[\"bbox\"]\n",
    "            seg = box[\"segmentation\"][0]\n",
    "            category_id = box[\"category_id\"]\n",
    "            label[:, category_id, int(y):int(y + height), int(x):int(x + width)] = 1\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(img)\n",
    "        loss = criterion(outputs, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "        pbar.set_description(\"loss: {:.4f}\".format(np.mean(losses)))\n",
    "    return losses\n",
    "\n",
    "def test_coco_semantic_segmentation(model, criterion, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for img, target in test_loader:\n",
    "            img, target = img.to(device), target.to(device)\n",
    "            label = torch.stack([torch.zeros_like(img)] * 90, dim=1)\n",
    "            for box in target:\n",
    "                x, y, width, height = box[\"bbox\"]\n",
    "                seg = box[\"segmentation\"][0]\n",
    "                category_id = box[\"category_id\"]\n",
    "                label[:, category_id, int(y):int(y + height), int(x):int(x + width)] = 1\n",
    "            outputs = model(img)\n",
    "            test_loss += criterion(outputs, label).item() * img.size(0)\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    return test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_losses = []\n",
    "deformable_losses = []\n",
    "\n",
    "for epoch in range(1, 11):\n",
    "    print(\"epoch {}\".format(epoch))\n",
    "    loss = train_coco_semantic_segmentation(unet_normal, unet_normal_optimizer, criterion, train_loader)\n",
    "    normal_losses.extend(loss)\n",
    "    loss = test_coco_semantic_segmentation(unet_normal, criterion, val_loader)\n",
    "    print('Test set: Average loss: {:.4f}'.format(loss))\n",
    "\n",
    "    loss = train_coco_semantic_segmentation(unet_deformable, unet_deformable_optimizer, criterion, train_loader)\n",
    "    deformable_losses.extend(loss)\n",
    "    loss = test_coco_semantic_segmentation(unet_deformable, criterion, val_loader)\n",
    "    print('Test set: Average loss: {:.4f}'.format(loss))\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(np.arange(1, 11), normal_losses, label='normal')\n",
    "plt.plot(np.arange(1, 11), deformable_losses, label='deformable')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sum(p.numel() for p in unet_normal.parameters() if p.requires_grad))\n",
    "print(sum(p.numel() for p in unet_deformable.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "- Deformable train IoU: 0.65\n",
    "- Normal train IoU: 0.62\n",
    "- Deformable test IoU: 0.54\n",
    "- Normal test IoU: 0.48\n",
    "\n",
    "Deformable results in more generalization and better performance with a little parameter overhead. Although each iteration costs more. Each epoch lasted nearly 5:20 while the normal one lasted 3:30."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
